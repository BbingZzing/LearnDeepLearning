{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils\n",
    "\n",
    "- `logit(u)`: í™•ë¥ ê°’(0~1)ì„ log-oddsë¡œ ë³€í™˜í•˜ëŠ” log transformì„ ìˆ˜í–‰(Loss ê³„ì‚° ì‹œ ìˆ˜í–‰)\n",
    "    \n",
    "    - $logit(u) = log \\frac{u}{1-u}$\n",
    "\n",
    "- `get_norms(model)`: ëª¨ë¸ weightì˜ L2norm(ìœ í´ë¦¬ë“œ ê±°ë¦¬)ê³„ì‚°. ê° ê°€ì¤‘ì¹˜ì™€ ê°€ì¤‘ì¹˜ gradientì˜ L2normì„ ê³„ì‚°í•˜ê³ , ë³€í™”ëŸ‰ í™•ì¸ì„ ìœ„í•´ ì‚¬ìš©.\n",
    "\n",
    "    1. `model.named_parameters()`ë¥¼ ì´ìš©í•´ ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "    2. ê° íŒŒë¼ë¯¸í„°(`param`)ì— ëŒ€í•´ L2 norm($\\sqrt{\\sum w^2}/ elements$)ì„ ê³„ì‚°\n",
    "\n",
    "    3. ê°€ì¤‘ì¹˜(`param.grad`)ì— ëŒ€í•´ gradient normì„ ê³„ì‚°\n",
    "\n",
    "    4. ë§Œì•½ `param.grad`ê°€ ì—†ìœ¼ë©´ `grad_norm = 0.0` ì„¤ì •\n",
    "\n",
    "    5. (`name`, `grad_norm`) í˜•íƒœë¡œ ë¦¬ìŠ¤íŠ¸ì—  ì €ì¥í•˜ì—¬ ë°˜í™˜\n",
    "\n",
    "- `create_log_dir(args, model_id)`: ë¡œê·¸ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬ë¥¼ ìƒì„±í•˜ê³ , ëª¨ë¸ IDì™€ ì‹¤í–‰ ì‹œê°„ì„ í´ë”ëª…ìœ¼ë¡œ í•˜ê³ , í´ë”ëª… ë°˜í™˜\n",
    "\n",
    "    - `args.suffix`: ëª¨ë¸ ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get log transform\n",
    "def logit(u):\n",
    "    return torch.log(u / (1. - u))\n",
    "\n",
    "### log transform at input numpy array\n",
    "def logit_np(u):\n",
    "    return torch.log(torch.tensor(u, dtype=torch.float32) / (1. - torch.tensor(u, dtype=torch.float32)))\n",
    "\n",
    "### calculate L2norm of gradient of weights\n",
    "def get_norms(model):\n",
    "    \"\"\"\n",
    "    í…ì„œì˜ ì›ì†Œ ë‹¹ ê°€ì¤‘ì¹˜ ë° ê°€ì¤‘ì¹˜ì˜ ê¸°ìš¸ê¸°ì— ëŒ€í•œ ì •ê·œí™”(L2norm)ë¥¼ ê³„ì‚°\n",
    "    \"\"\"\n",
    "    norms = [] # ê°€ì¤‘ì¹˜ì˜ L2norm ì €ì¥ ì»¨í…Œì´ë„ˆ\n",
    "    grad_norms = [] # ê°€ì¤‘ì¹˜ gradientì˜ L2norm ì €ì¥ ì»¨í…Œì´ë„ˆ(ì—†ëŠ” ê²½ìš° 0ìœ¼ë¡œ ì €ì¥)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        # ê°€ì¤‘ì¹˜ì˜ L2 norm ê³„ì‚°\n",
    "        norm = torch.sqrt(torch.sum(torch.square(param))) / torch.prod(torch.tensor(param.shape, dtype=torch.float32))\n",
    "        norms.append(norm)\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜ ê°’ì„ ê°€ì§€ëŠ” ê²½ìš°, ê°€ì¤‘ì¹˜ gradient L2norm ê³„ì‚°\n",
    "        if param.grad is not None:\n",
    "            grad_norm = torch.sqrt(torch.sum(torch.square(param.grad))) / torch.prod(torch.tensor(param.shape, dtype=torch.float32))\n",
    "            grad_norms.append(grad_norm)\n",
    "        # ê°€ì¤‘ì¹˜ ê°’ì´ ì—†ëŠ” ê²½ìš°, grad_norms ë¦¬ìŠ¤íŠ¸ì— 0 ì¶”ê°€\n",
    "        else:\n",
    "            grad_norms.append(torch.tensor(0.0))\n",
    "    \n",
    "    return norms, grad_norms\n",
    "\n",
    "### create log directory\n",
    "def create_log_dir(args, model_id):\n",
    "    model_id += args.suffix + time.strftime('-%y%m%dT%H%M%S') # ID + time\n",
    "    model_dir = os.path.join(os.path.expanduser(args.output_dir), model_id) \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression\n",
    "\n",
    "- `class LeackyRelu`: ì…ë ¥ê°’ì´ 0ë³´ë‹¤ í¬ë©´ ê·¸ëŒ€ë¡œ ì¶œë ¥, 0 ì´í•˜ì´ë©´ 0.05ë¥¼ ê³±í•œ ê°’ì„ ì¶œë ¥í•˜ëŠ” LeakyReLU í™œì„±í•¨ìˆ˜ë¥¼ êµ¬í˜„\n",
    "\n",
    "- `class MultiscaleConvolution`: ì…ë ¥ëœ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ í•´ìƒë„ë¡œ ë³€í™˜ì‹œí‚¤ê³ , í•©ì„±ê³± ë° ë¹„ì„ í˜• í™œì„±í™”ë¥¼ ìˆ˜í–‰í•˜ë©°, ê·¸ ê²°ê³¼ë¥¼ ì—…ìƒ˜í”Œë§í•˜ì—¬ ëˆ„ì  í‰ê· ì„ ê³„ì‚°\n",
    "\n",
    "    $\\rightarrow$ ë©€í‹°ìŠ¤ì¼€ì¼ Convë¥¼ ìˆ˜í–‰í•˜ëŠ” ì´ìœ ëŠ” timestep(`t`)ë³„ë¡œ ë…¸ì´ì¦ˆ ì¶”ê°€ë¡œ ì¸í•´ ë¶„ì„í•´ì•¼ í•  ì´ë¯¸ì§€ í•´ìƒë„ê°€ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸\n",
    "\n",
    "    (ì´ˆê¸°ì—ëŠ” ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼, í›„ê¸°ì—ëŠ” ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì´ë¯¸ì§€ì—ì„œ ì €í•´ìƒë„ ì´ë¯¸ì§€ íŒ¨í„´ì„ ë” ì¤‘ìš”ì‹œ í•´ì•¼ í•¨)\n",
    "\n",
    "    $\\rightarrow$ ì—¬ê¸°ì„œ Isotropic Gaussian ì´ˆê¸°í™”ë¥¼ í†µí•´ Markov Kernelì„ êµ¬í˜„\n",
    "\n",
    "    - **Params**\n",
    "\n",
    "        - `num_channels(int)`: ì…ë ¥ ì±„ë„ ìˆ˜\n",
    "\n",
    "        - `num_filters(int)`: ì¶œë ¥ ì±„ë„ ìˆ˜\n",
    "        \n",
    "        - `spatial_width(int)`: ì…ë ¥ ì´ë¯¸ì§€ì˜ ê³µê°„ì  í¬ê¸° (ì •ì‚¬ê°í˜•ì´ë¼ê³  ê°€ì •)\n",
    "\n",
    "        - `num_scales(int)`: ì‚¬ìš©í•  ìŠ¤ì¼€ì¼ì˜ ìˆ˜\n",
    "\n",
    "        - `filter_size (int)`: ì»¨ë³¼ë£¨ì…˜ í•„í„° í¬ê¸° (ì •ì‚¬ê°í˜•)\n",
    "\n",
    "        - `downsample_method (str)`: ë‹¤ìš´ìƒ˜í”Œë§ ë°©ì‹ ('meanout' ë“±, ì—¬ê¸°ì„œëŠ” í‰ê·  í’€ë§ ì‚¬ìš©)\n",
    "\n",
    "        - `name (str)`: ë ˆì´ì–´ ì´ë¦„(ë””ë²„ê¹…ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 1. LeakyReLU í™œì„±í™” í•¨ìˆ˜ \n",
    "# í´ë˜ìŠ¤ ë˜í¼(ì¡°ê±´ì— ë”°ë¼ MultiscaleConvolutionì˜ ë™ì‘ì„ ì œì–´)\n",
    "##############################################\n",
    "class LeakyRelu(nn.Module):\n",
    "    \"\"\"\n",
    "    ìŒì˜ ê¸°ìš¸ê¸°ë¥¼ ê°€ì§€ëŠ” LeakyReLU í•¨ìˆ˜.\n",
    "    ì…ë ¥ì´ 0ë³´ë‹¤ í¬ë©´ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ê³ , 0 ì´í•˜ì´ë©´ 0.05ë¥¼ ê³±í•œ ê°’ì„ ì¶œë ¥.\n",
    "    \"\"\"\n",
    "    def __init__(self, negative_slope=0.05):\n",
    "        super(LeakyRelu, self).__init__()\n",
    "        self.negative_slope = negative_slope\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.leaky_relu(input, negative_slope=self.negative_slope)\n",
    "\n",
    "# denseì™€ convolutionì—ì„œ ë™ì¼í•˜ê²Œ ì‚¬ìš©\n",
    "dense_nonlinearity = LeakyRelu(negative_slope=0.05)\n",
    "conv_nonlinearity = LeakyRelu(negative_slope=0.05)\n",
    "\n",
    "##############################################\n",
    "# 2. MultiScaleConvolution í´ë˜ìŠ¤\n",
    "##############################################\n",
    "class MultiScaleConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´.\n",
    "    \n",
    "    ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ìŠ¤ì¼€ì¼(í•´ìƒë„)ë¡œ ë³€í™˜í•œ í›„ ê° ìŠ¤ì¼€ì¼ì— ëŒ€í•´\n",
    "    ì»¨ë³¼ë£¨ì…˜(ë° ë¹„ì„ í˜• í™œì„±í™”)ì„ ìˆ˜í–‰í•˜ê³ , ê²°ê³¼ë¥¼ ì—…ìƒ˜í”Œë§í•˜ì—¬ ëˆ„ì  í‰ê· ì„ ê³„ì‚°\n",
    "    \n",
    "    Args:\n",
    "        num_channels (int): ì…ë ¥ ì±„ë„ ìˆ˜\n",
    "        num_filters (int): ì¶œë ¥ ì±„ë„(í•„í„°) ìˆ˜\n",
    "        spatial_width (int): ì…ë ¥ ì´ë¯¸ì§€ì˜ ê³µê°„ì  í¬ê¸° (ì •ì‚¬ê°í˜•ì´ë¼ê³  ê°€ì •)\n",
    "        num_scales (int): ì‚¬ìš©í•  ìŠ¤ì¼€ì¼ì˜ ìˆ˜\n",
    "        filter_size (int): ì»¨ë³¼ë£¨ì…˜ í•„í„° í¬ê¸° (ì •ì‚¬ê°í˜•)\n",
    "        downsample_method (str): ë‹¤ìš´ìƒ˜í”Œë§ ë°©ì‹ ('meanout' ë“±, ì—¬ê¸°ì„œëŠ” í‰ê·  í’€ë§ ì‚¬ìš©)\n",
    "        name (str): ë ˆì´ì–´ ì´ë¦„(ë””ë²„ê¹…ìš©)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, num_filters, spatial_width, num_scales, filter_size, downsample_method='meanout', name=\"\"):\n",
    "        super(MultiScaleConvolution, self).__init__()\n",
    "        self.num_scales = num_scales\n",
    "        self.filter_size = filter_size\n",
    "        self.num_filters = num_filters\n",
    "        self.spatial_width = spatial_width\n",
    "        self.downsample_method = downsample_method\n",
    "        self.name = name\n",
    "        # 'overshoot': full Conv.ë¡œ ì¸í•´ ìƒê¸°ëŠ” ì—¬ë¶„ì˜ ê°€ì¥ìë¦¬ ìˆ˜\n",
    "        self.overshoot = (filter_size - 1) // 2\n",
    "\n",
    "        # ê° ìŠ¤ì¼€ì¼ë§ˆë‹¤ ì»¨ë³¼ë£¨ì…˜+í™œì„±í™” ë¸”ë¡ ìƒì„± (ModuleListì— ì €ì¥)\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for scale in range(num_scales):\n",
    "            # PyTorchì—ì„œëŠ” 'full Convolution'ì„ padding=filter_size-1 ë¡œ í‰ë‚´ ë‚¼ ìˆ˜ ìˆìŒ.\n",
    "            conv = nn.Conv2d(\n",
    "                in_channels=num_channels,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=filter_size,\n",
    "                padding=filter_size - 1  # full convolution íš¨ê³¼\n",
    "            )\n",
    "            # ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”ì— IsotropicGaussian ì ìš©\n",
    "            # DPMì—ì„œëŠ” ëª¨ë“  í”½ì…€ì— ëŒ€í•´ ê· ë“±í•œ ë…¸ì´ì¦ˆë¥¼ í•™ìŠµí•˜ë„ë¡ í•˜ëŠ” ì—­í•  ìˆ˜í–‰\n",
    "            std = torch.sqrt(1.0 / num_filters) / (filter_size ** 2)\n",
    "            self.isotropic_gaussian_init(conv.weight, std=std)\n",
    "            \n",
    "            # bias ì´ˆê¸°í™”\n",
    "            nn.init.constant_(conv.bias, 0)  # Biasë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "            \n",
    "            # ì»¨ë³¼ë£¨ì…˜ í›„ ë¹„ì„ í˜• í™œì„±í™” ì ìš©\n",
    "            layer = nn.Sequential(conv, conv_nonlinearity)\n",
    "            # ModuleListì— í•©ì„±ê³±+LeakyReLU ì €ì¥\n",
    "            self.conv_layers.append(layer)\n",
    "\n",
    "    def isotropic_gaussian_init(self, tensor, std=1.0):\n",
    "        \"\"\" Isotropic Gaussian ì´ˆê¸°í™” \"\"\"\n",
    "        shape = tensor.shape\n",
    "        num_elements = tensor.numel()  # ì „ì²´ ì›ì†Œ ê°œìˆ˜\n",
    "        identity_cov = torch.eye(num_elements) * (std ** 2)  # ê³µë¶„ì‚° í–‰ë ¬: I * std^2\n",
    "\n",
    "        # ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ ìƒ˜í”Œë§\n",
    "        w = torch.distributions.MultivariateNormal(torch.zeros(num_elements), identity_cov).sample()\n",
    "\n",
    "        # ì›ë˜ í…ì„œ í˜•íƒœë¡œ ë³€í˜•\n",
    "        tensor.data = w.view(shape)\n",
    "        \n",
    "    def downsample(self, imgs, scale):\n",
    "        \"\"\"\n",
    "        ì´ë¯¸ì§€ í…ì„œë¥¼ ì£¼ì–´ì§„ ìŠ¤ì¼€ì¼ë§Œí¼ í‰ê·  í’€ë§ìœ¼ë¡œ ë‹¤ìš´ìƒ˜í”Œë§.\n",
    "        Diffusion Mechanismì—ì„œ coarse-to-fine ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ë„ë¡...\n",
    "        \n",
    "        Args:\n",
    "            imgs (Tensor): (batch, channels, height, width) í˜•íƒœì˜ ì´ë¯¸ì§€ í…ì„œ\n",
    "            scale (int): ë‹¤ìš´ìƒ˜í”Œë§ ìŠ¤ì¼€ì¼ (2**scale ë§Œí¼ ì¶•ì†Œ)\n",
    "            \n",
    "        Returns:\n",
    "            ë‹¤ìš´ìƒ˜í”Œë§ëœ í…ì„œ\n",
    "        \"\"\"\n",
    "        if scale == 0:\n",
    "            return imgs\n",
    "        kernel_size = 2 ** scale\n",
    "        return F.avg_pool2d(imgs, kernel_size=kernel_size, stride=kernel_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        ì…ë ¥ ì´ë¯¸ì§€ì— ëŒ€í•´ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì»¨ë³¼ë£¨ì…˜ì„ ì ìš©í•˜ê³ ,\n",
    "        ê° ìŠ¤ì¼€ì¼ì˜ ê²°ê³¼ë¥¼ ì—…ìƒ˜í”Œí•˜ì—¬ ëˆ„ì í•œ í›„ í‰ê· ì„ ë°˜í™˜.\n",
    "        \n",
    "        Args:\n",
    "            X (Tensor): (batch, channels, spatial_width, spatial_width) í˜•íƒœ.\n",
    "            \n",
    "        Returns:\n",
    "            ìŠ¤ì¼€ì¼ í‰ê· í™”ëœ ê²°ê³¼ í…ì„œ (ì…ë ¥ê³¼ ë™ì¼í•œ ê³µê°„ í¬ê¸°).\n",
    "        \"\"\"\n",
    "        acc = None  # ê²°ê³¼ ëˆ„ì  ë³€ìˆ˜\n",
    "        # ìŠ¤ì¼€ì¼ì„ ê±°ì¹ ìˆ˜ë¡ í•´ìƒë„ê°€ ë‚®ì•„ì§€ë¯€ë¡œ, coarsest (ìµœì € í•´ìƒë„)ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰\n",
    "        for scale in reversed(range(self.num_scales)):\n",
    "            # ì…ë ¥ ì´ë¯¸ì§€ë¥¼ í•´ë‹¹ ìŠ¤ì¼€ì¼ë¡œ ë‹¤ìš´ìƒ˜í”Œë§\n",
    "            X_down = self.downsample(X, scale)\n",
    "            # í•´ë‹¹ ìŠ¤ì¼€ì¼ì˜ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ ì ìš©\n",
    "            out = self.conv_layers[scale](X_down)\n",
    "            # full ì»¨ë³¼ë£¨ì…˜ìœ¼ë¡œ ì¸í•œ ì—¬ë¶„ì˜ ê°€ì¥ìë¦¬(crop)ë¥¼ ì œê±°í•˜ì—¬ ì›ë˜ í¬ê¸°ë¡œ ë§ì¶¤\n",
    "            if self.overshoot > 0:\n",
    "                out = out[:, :, self.overshoot:-self.overshoot, self.overshoot:-self.overshoot]\n",
    "            # ëˆ„ì í•©ì‚°: ì´ˆê¸°ì—ëŠ” ë‹¨ìˆœíˆ í• ë‹¹, ì´í›„ì—ëŠ” ì´ì „ ê²°ê³¼ì™€ ë”í•¨.\n",
    "            if acc is None:\n",
    "                acc = out\n",
    "            else:\n",
    "                acc = acc + out\n",
    "            # í˜„ì¬ ìŠ¤ì¼€ì¼ì´ ìµœìƒí•´ìƒë„ê°€ ì•„ë‹ˆë©´, ë‹¤ìŒ ìŠ¤ì¼€ì¼ê³¼ ë§ì¶”ê¸° ìœ„í•´ 2ë°° ì—…ìƒ˜í”Œë§ (ìµœê·¼ì ‘ ë³´ê°„)\n",
    "            if scale > 0:\n",
    "                acc = F.interpolate(acc, scale_factor=2, mode='nearest')\n",
    "        # ê° ìŠ¤ì¼€ì¼ì˜ ê²°ê³¼ í‰ê· ì„ ë°˜í™˜\n",
    "        return acc / self.num_scales\n",
    "\n",
    "##############################################\n",
    "# 3. MultiLayerConvolution í´ë˜ìŠ¤\n",
    "##############################################\n",
    "class MultiLayerConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ ê°œì˜ MultiScaleConvolution ë ˆì´ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìŒ“ì€ ëª¨ë“ˆ.\n",
    "    \n",
    "    Args:\n",
    "        n_layers (int): ì‚¬ìš©í•  MultiScaleConvolution ë ˆì´ì–´ ìˆ˜.\n",
    "        n_hidden (int): ê° ë ˆì´ì–´ì˜ ì¶œë ¥ ì±„ë„ ìˆ˜.\n",
    "        spatial_width (int): ì…ë ¥ ì´ë¯¸ì§€ì˜ ê³µê°„ í¬ê¸°.\n",
    "        n_colors (int): ì…ë ¥ ì´ë¯¸ì§€ì˜ ì±„ë„ ìˆ˜.\n",
    "        n_scales (int): ê° MultiScaleConvolution ë ˆì´ì–´ì—ì„œ ì‚¬ìš©í•  ìŠ¤ì¼€ì¼ ìˆ˜.\n",
    "        filter_size (int): ì»¨ë³¼ë£¨ì…˜ í•„í„° í¬ê¸°.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, n_hidden, spatial_width, n_colors, n_scales, filter_size=3):\n",
    "        super(MultiLayerConvolution, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        num_channels = n_colors\n",
    "        for i in range(n_layers):\n",
    "            layer = MultiScaleConvolution(\n",
    "                num_channels=num_channels,\n",
    "                num_filters=n_hidden,\n",
    "                spatial_width=spatial_width,\n",
    "                num_scales=n_scales,\n",
    "                filter_size=filter_size,\n",
    "                name=f\"layer{i}_\"\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "            num_channels = n_hidden  # ë‹¤ìŒ ë ˆì´ì–´ì˜ ì…ë ¥ ì±„ë„ì€ ì´ì „ ë ˆì´ì–´ì˜ ì¶œë ¥ ì±„ë„\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        ìˆœì°¨ì ìœ¼ë¡œ ê° MultiScaleConvolution ë ˆì´ì–´ë¥¼ ì ìš©.\n",
    "        \n",
    "        Args:\n",
    "            X (Tensor): ì…ë ¥ ì´ë¯¸ì§€ (batch, n_colors, spatial_width, spatial_width)\n",
    "            \n",
    "        Returns:\n",
    "            ìµœì¢… ì»¨ë³¼ë£¨ì…˜ ê²°ê³¼.\n",
    "        \"\"\"\n",
    "        out = X\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "\n",
    "##############################################\n",
    "# 4. MLP ìƒì„± í—¬í¼ í•¨ìˆ˜\n",
    "##############################################\n",
    "def make_mlp(layer_dims, activations):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ì¸µ í¬ê¸°ì™€ í™œì„±í™” í•¨ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ MLP(ì™„ì „ì—°ê²° ì‹ ê²½ë§)ë¥¼ êµ¬ì„±.\n",
    "    \n",
    "    Args:\n",
    "        layer_dims (list of int): ê° ì¸µì˜ í¬ê¸° (ì˜ˆ: [input_dim, hidden_dim, output_dim]).\n",
    "        activations (list of nn.Module): ê° ì¸µì— ì ìš©í•  í™œì„±í™” í•¨ìˆ˜.\n",
    "            (ì¶œë ¥ì¸µì—ëŠ” ë³´í†µ í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ê±°ë‚˜ nn.Identity() ì‚¬ìš©)\n",
    "            \n",
    "    Returns:\n",
    "        nn.Sequential: êµ¬ì„±ëœ MLP ëª¨ë“ˆ.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for i in range(len(layer_dims) - 1):\n",
    "        layers.append(nn.Linear(layer_dims[i], layer_dims[i + 1]))\n",
    "        if i < len(activations):\n",
    "            layers.append(activations[i])\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "##############################################\n",
    "# 5. MLP_conv_dense í´ë˜ìŠ¤\n",
    "##############################################\n",
    "class MLP_conv_dense(nn.Module):\n",
    "    \"\"\"\n",
    "    ì…ë ¥ ì´ë¯¸ì§€ì—ì„œ ì‹œê°„ì  ê³„ìˆ˜ë¥¼ ë°˜ì˜í•˜ê³ , ì „ì—­ì  íŠ¹ì„± ë°˜ì˜ì„ ìœ„í•œ Convolution/ì§€ì—­ì ì¸ íŠ¹ì„± ë°˜ì˜ì„ ìœ„í•œ MLP ëª¨ë“ˆì„ ì‚¬ìš©.\n",
    "    ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë‘ ê°œì˜ ë¶„ê¸°ê°€ ìˆìŒ:\n",
    "      1) í•˜ë¶€ ë¶„ê¸°: ì»¨ë³¼ë£¨ì…˜ ê¸°ë°˜ MLP (MultiLayerConvolution)ì™€ (ì„ íƒì ìœ¼ë¡œ) ì™„ì „ì—°ê²° MLP\n",
    "      => CNNìœ¼ë¡œ ê³µê°„ì  íŠ¹ì§• ì¶”ì¶œ -> ì „ì²´ ì´ë¯¸ì§€ë¥¼ 1D ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ í•™ìŠµ\n",
    "      2) ìƒë¶€ ë¶„ê¸°: ê° í”½ì…€ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë˜ëŠ” ì™„ì „ì—°ê²° MLP (ì‹¤ì œë¡œ 1x1 ì»¨ë³¼ë£¨ì…˜ê³¼ ìœ ì‚¬)\n",
    "      => mu, sigma ì˜ˆì¸¡ì„ ìœ„í•œ í•™ìŠµ\n",
    "    \n",
    "    Args:\n",
    "        n_layers_conv (int): ì»¨ë³¼ë£¨ì…˜ ë¶„ê¸°ì˜ ë ˆì´ì–´ ìˆ˜.\n",
    "        n_layers_dense_lower (int): í•˜ë¶€ ì™„ì „ì—°ê²° MLPì˜ ë ˆì´ì–´ ìˆ˜.\n",
    "        n_layers_dense_upper (int): ìƒë¶€ ì™„ì „ì—°ê²° MLPì˜ ë ˆì´ì–´ ìˆ˜.\n",
    "        n_hidden_conv (int): ì»¨ë³¼ë£¨ì…˜ ë¶„ê¸°ì˜ ì€ë‹‰ ì±„ë„ ìˆ˜.\n",
    "        n_hidden_dense_lower (int): í•˜ë¶€ ì™„ì „ì—°ê²° MLPì˜ ì€ë‹‰ ìœ ë‹› ìˆ˜.\n",
    "        n_hidden_dense_lower_output (int): í•˜ë¶€ ì™„ì „ì—°ê²° MLPì˜ ì¶œë ¥ ì±„ë„ ìˆ˜ (ê³µê°„ ì „ì²´ì— ëŒ€í•´).\n",
    "        n_hidden_dense_upper (int): ìƒë¶€ ì™„ì „ì—°ê²° MLPì˜ ì€ë‹‰ ìœ ë‹› ìˆ˜.\n",
    "        spatial_width (int): ì…ë ¥ ì´ë¯¸ì§€ì˜ ê³µê°„ í¬ê¸°.\n",
    "        n_colors (int): ì…ë ¥ ì´ë¯¸ì§€ì˜ ì±„ë„ ìˆ˜.\n",
    "        n_scales (int): ì»¨ë³¼ë£¨ì…˜ ë¶„ê¸°ì—ì„œ ì‚¬ìš©í•  ìŠ¤ì¼€ì¼ ìˆ˜.\n",
    "        n_temporal_basis (int): ì‹œê°„ì  ê¸°ì €(temporal basis)ì˜ ìˆ˜ (ì¶œë ¥ ê³„ìˆ˜ì— ì˜í–¥ì„ ì¤Œ).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers_conv, n_layers_dense_lower, n_layers_dense_upper,\n",
    "                 n_hidden_conv, n_hidden_dense_lower, n_hidden_dense_lower_output, \n",
    "                 n_hidden_dense_upper, spatial_width, n_colors, n_scales, n_temporal_basis):\n",
    "        super(MLP_conv_dense, self).__init__()\n",
    "        self.n_colors = n_colors\n",
    "        self.spatial_width = spatial_width\n",
    "        self.n_hidden_conv = n_hidden_conv\n",
    "        self.n_hidden_dense_lower = n_hidden_dense_lower\n",
    "        self.n_hidden_dense_lower_output = n_hidden_dense_lower_output\n",
    "        self.n_layers_dense_lower = n_layers_dense_lower  # ì„ íƒì  ë¶„ê¸° ì¡´ì¬ ì—¬ë¶€ íŒë‹¨ìš©\n",
    "\n",
    "        # í•˜ë¶€ ë¶„ê¸° - ì»¨ë³¼ë£¨ì…˜ ê¸°ë°˜ MLP\n",
    "        self.mlp_conv = MultiLayerConvolution(n_layers_conv, n_hidden_conv, spatial_width, n_colors, n_scales)\n",
    "\n",
    "        # í•˜ë¶€ ë¶„ê¸° - ì™„ì „ì—°ê²° MLP (ì„ íƒì )\n",
    "        if n_hidden_dense_lower > 0 and n_layers_dense_lower > 0:\n",
    "            n_input = n_colors * (spatial_width ** 2)\n",
    "            n_output = n_hidden_dense_lower_output * (spatial_width ** 2)\n",
    "            activations = [dense_nonlinearity] * (n_layers_dense_lower - 1)\n",
    "            self.mlp_dense_lower = make_mlp(\n",
    "                [n_input] + [n_hidden_dense_lower] * (n_layers_dense_lower - 1) + [n_output],\n",
    "                activations\n",
    "            )\n",
    "        else:\n",
    "            self.mlp_dense_lower = None\n",
    "            self.n_hidden_dense_lower_output = 0\n",
    "\n",
    "        # ìƒë¶€ ë¶„ê¸° - ê° í”½ì…€ë§ˆë‹¤ ì ìš©ë˜ëŠ” ì™„ì „ì—°ê²° MLP (ì¶œë ¥: muì™€ sigma ê°ê°ì— ëŒ€í•´)\n",
    "        n_output = n_colors * n_temporal_basis * 2  # muì™€ sigma ë‘ ê°’ì„ ìœ„í•´ *2\n",
    "        # ìƒë¶€ MLPì˜ í™œì„±í™”: ë§ˆì§€ë§‰ ì¸µì€ í•­ë“±í•¨ìˆ˜(nn.Identity()) ì‚¬ìš©\n",
    "        activations_upper = [dense_nonlinearity] * (n_layers_dense_upper - 1) + [nn.Identity()]\n",
    "        self.mlp_dense_upper = make_mlp(\n",
    "            [n_hidden_conv + self.n_hidden_dense_lower_output] + [n_hidden_dense_upper] * (n_layers_dense_upper - 1) + [n_output],\n",
    "            activations_upper\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        noisy ì…ë ¥ ì´ë¯¸ì§€ë¡œë¶€í„° ì‹œê°„ì  ê³„ìˆ˜(temporal coefficients)ë¥¼ ìƒì„±.\n",
    "        \n",
    "        ìˆœì„œ:\n",
    "          1) ì»¨ë³¼ë£¨ì…˜ ë¶„ê¸°ë¥¼ í†µí•´ íŠ¹ì§• ì¶”ì¶œ (mlp_conv).\n",
    "          2) íŠ¹ì§• ì°¨ì› ë³€ê²½: (batch, channels, H, W) â†’ (batch, H, W, channels).\n",
    "          3) (ì„ íƒì ) í•˜ë¶€ ì™„ì „ì—°ê²° ë¶„ê¸°ë¥¼ í†µí•´ ì „ì—­ ì •ë³´ë¥¼ ì¶”ì¶œí•œ í›„, ì»¨ë³¼ë£¨ì…˜ ë¶„ê¸°ì˜ ê²°ê³¼ì™€ ê²°í•©.\n",
    "          4) ìƒë¶€ ì™„ì „ì—°ê²° MLPë¥¼ ê° í”½ì…€ì— ëŒ€í•´ ì ìš©í•˜ì—¬ ìµœì¢… ì¶œë ¥ì„ ìƒì„±.\n",
    "          \n",
    "        Args:\n",
    "            X (Tensor): (batch, n_colors, spatial_width, spatial_width) í¬ê¸°ì˜ ì…ë ¥ ì´ë¯¸ì§€.\n",
    "            \n",
    "        Returns:\n",
    "            ìµœì¢… ì¶œë ¥ í…ì„œ (batch, spatial_width, spatial_width, n_output)\n",
    "        \"\"\"\n",
    "        # 1) ì»¨ë³¼ë£¨ì…˜ ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ\n",
    "        Y = self.mlp_conv(X)  # ì‹œê°„(t)ì—ì„œ ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì´ë¯¸ì§€ ì…ë ¥\n",
    "                              # (batch, n_hidden_conv, spatial_width, spatial_width)\n",
    "        # 2) ì±„ë„ ì°¨ì›ì„ ë§ˆì§€ë§‰ ì¶•ìœ¼ë¡œ ì´ë™\n",
    "        Y = Y.permute(0, 2, 3, 1)  # (batch, spatial_width, spatial_width, n_hidden_conv)\n",
    "        \n",
    "        # 3) ì„ íƒì  í•˜ë¶€ ì™„ì „ì—°ê²° ë¶„ê¸° ì ìš© ë° íŠ¹ì§• ê²°í•©\n",
    "        if self.mlp_dense_lower is not None:\n",
    "            batch_size = X.size(0)\n",
    "            X_flat = X.view(batch_size, -1)  # (batch, n_colors * spatial_width^2)\n",
    "            Y_dense = self.mlp_dense_lower(X_flat)  # (batch, n_hidden_dense_lower_output * spatial_width^2)\n",
    "            Y_dense = Y_dense.view(batch_size, self.spatial_width, self.spatial_width, self.n_hidden_dense_lower_output)\n",
    "            # ê° ë¶„ê¸°ì˜ ê²°ê³¼ë¥¼ ì •ê·œí™”í•˜ì—¬ ê²°í•© (ìŠ¤ì¼€ì¼ ì¡°ì •ì„ ìœ„í•´ ê°ê° sqrt(ìœ ë‹›ìˆ˜)ë¡œ ë‚˜ëˆ”)\n",
    "            Y = torch.cat([\n",
    "                Y / math.sqrt(self.n_hidden_conv),\n",
    "                Y_dense / math.sqrt(self.n_hidden_dense_lower_output)\n",
    "            ], dim=3)\n",
    "\n",
    "        # 4) ìƒë¶€ MLPë¥¼ ê° í”½ì…€ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ì ìš© (í”½ì…€ ë‹¨ìœ„ MLP. 1x1 ì»¨ë³¼ë£¨ì…˜ê³¼ ìœ ì‚¬)\n",
    "        batch_size, H, W, channels = Y.shape\n",
    "        Y_flat = Y.view(batch_size * H * W, channels)\n",
    "        Z_flat = self.mlp_dense_upper(Y_flat)\n",
    "        Z = Z_flat.view(batch_size, H, W, -1)\n",
    "        return Z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model\n",
    "\n",
    "- `generate_beta_arr`:Diffusion ê³¼ì •ì—ì„œ timestep(`t`)ì˜ ìˆ˜ë§Œí¼ `beta_t`ê°’ì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ\n",
    "\n",
    "    - `beta_t`ëŠ” $T_\\pi (y | y'; \\beta) = N(y;\\sqrt{1-\\beta}\\cdot y', \\beta I)$ ì— ë”°ë¼ ê° ì‹œê°„(`t`)ì—ì„œ ë…¸ì´ì¦ˆì˜ í¬ê¸°ë¥¼ ë‚˜íƒ€ëƒ„.\n",
    "\n",
    "    - ì´ˆê¸° ì…ë ¥ê°’ì€ `step1_beta`ì´ë©°, ì‹œê°„ì  ê¸°ì € í•¨ìˆ˜(`__init__`ì—ì„œ `generate_temporal_basis`í•¨ìˆ˜ì— ì˜í•´ ì •ì˜)ì— ì˜í•´ ì •ì˜ë˜ê³ , íŒŒë¼ë¯¸í„°ì¸ `beta_perturb_coefficients_values`ì™€ ê³±í•´ì ¸ ê³„ì‚°ëœ `beta_perturb`ê°€ ì¦ê°€í•˜ëŠ” `\n",
    "\n",
    "- `get_t_weights`: íŠ¹ì • ì‹œì (`t`)ì„ ë§¤ê°œë³€ìˆ˜ë¡œ ì…ë ¥ë°›ì•„ í•´ë‹¹ Indexë§Œ 1ì´ê³ , ë‚˜ë¨¸ì§€ëŠ” 0ì¸ 1ì°¨ì› í…ì„œë¥¼ ë°˜í™˜\n",
    "\n",
    "    - t ì‹œì ì˜ ì •ë³´ë¥¼ ë½‘ì•„ë‚´ëŠ” ì—­í• ì„ í•˜ë©°, ë‹¤ë¥¸ í•¨ìˆ˜ì—ì„œ ì‚¬ìš©ë  ë•Œ í•´ë‹¹ ì‹œì ë§Œ ì„ íƒ\n",
    "\n",
    "- `get_beta_forward`: íŠ¹ì • ì‹œì (`t`)ë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ì…ë ¥ë°›ì•„ í•´ë‹¹ Indexë§Œ beta_t ê°’ì„ ê°€ì§€ëŠ” timestep ìˆ˜ ë§Œí¼ì˜ 1ì°¨ì› í…ì„œ ë°˜í™˜\n",
    "\n",
    "- `get_mu_sigma`: ì—­ë°©í–¥ ê³¼ì •ì—ì„œì˜ í‰ê·  `Î¼_t`ì™€ í‘œì¤€í¸ì°¨ `Ïƒ_t`ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    - forward diffusionì—ì„œì˜ ë…¸ì´ì¦ˆ ì¶”ê°€ ë¶„í¬ ì‹: $x^{(t)} = \\sqrt{1-\\beta_t} x^{(t-1)} + \\sqrt{\\beta_t}\\epsilon, \\quad \\epsilon \\thicksim N(0, I)$\n",
    "\n",
    "        1) ìœ„ ì‹ì€ $x^{(t)} - \\sqrt{\\beta_t -1} x^{(t-1)} = \\sqrt{\\beta_t}\\epsilon$ ìœ¼ë¡œ ë³€í˜• ê°€ëŠ¥\n",
    "\n",
    "        2) ìœ„ ì‹ì—ì„œ ìš°ì¸¡ í•­ì€ í‘œì¤€ ì •ê·œë¶„í¬(Gaussian distribution)ì„ ë”°ë¥´ë¯€ë¡œ $(x^{(t)} - \\sqrt{\\beta_t -1} x^{(t-1)})\\thicksim N(0, I)$ ì™€ ê°™ì´ í‘œí˜„ ê°€ëŠ¥\n",
    "\n",
    "        3) ì¼ë°˜ Gaussian Distributionì˜ ë¶„í¬ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜ëŠ” $p(x) = \\frac {1} {\\sqrt{2 \\pi \\sigma^2}} exp\\big(- \\frac{(x-\\mu)^2} {2 \\sigma^2} \\big)$ ì´ê³ , forward diffusion ë¶„í¬ ì‹ì„ í™•ë£° ë°€ë„í•¨ìˆ˜(P.D.F.)í˜•íƒœë¡œ í’€ì–´ì“°ë©´ $p(x^{(t)}|x^{(t-1)}) = \\frac {1} {\\sqrt{2 \\pi \\beta_t}} exp\\big( -\\frac{(x^{(t)}-\\sqrt{\\beta_t x^{(t-1)}})^2}{2 \\beta_t} \\big)$ ê°€ ëœë‹¤.\n",
    "\n",
    "        4) ë”°ë¼ì„œ í‰ê·  $(\\mu) = \\frac {1} {\\sqrt{2 \\pi \\beta_t}} $, ë¶„ì‚° $(\\sigma^2) = \\beta_t$ ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆìŒ\n",
    "\n",
    "    - reverse diffusionì—ì„œì˜ ë…¸ì´ì¦ˆ ì¶”ê°€ ë¶„í¬ ìœ ë„(`Î¼_t` ë° `Ïƒ_t` ìœ ë„ê³¼ì •)\n",
    "\n",
    "        1) ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ì´ìš©í•œ reverse diff. í™•ë¥ ë¶„í¬: $p(x^{(t-1)}|x^{(t)}) = p(x^{(t)}|x^{(t-1)})p(x^{(t-1)})/p(x^{(t)})$\n",
    "\n",
    "        2) ì´ì „ timestep(`t-1`)ì—ì„œì˜ ë°ì´í„°($x^{(t-1)}$)ëŠ” ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ë©´, **ì‚¬ì „(Prior)í™•ë¥ ë¶„í¬**ëŠ” $ p^{(x^{(t-1)})} = N(x^{(t-1)}; 0, \\sigma^2_{t-1}I) $ ì´ê³ , ì´ë¥¼ ì¼ë°˜ì ì¸ Gaussian Distribution í˜•íƒœë¡œ í‘œí˜„í•˜ë©´ $p(x^{(t-1)}) = 1 / \\sqrt{2\\pi \\sigma^2_{t-1}} exp \\big(  - \\frac{x^2_{t-1}}{2 \\sigma^2_{t-1}} \\big) $ ì„\n",
    "\n",
    "        3) `1)`ì— ì˜í•´ $p(x^{(t-1)}|x^{(t)}) \\propto p(x^{(t)}|x^{(t-1)})p(x^{(t-1)}) $ ì™€ ê°™ì€ ê´€ê³„ë¥¼ ê°€ì§€ë¯€ë¡œ, **ì‚¬í›„(posterior)í™•ë¥ **ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” $p(x^{(t)}|x^{(t-1)})$ ì™€ $p(x^{(t-1)})$ ë‘ ê°œì˜ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ê³±í•´ì„œ ìƒˆë¡œìš´ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŒ\n",
    "\n",
    "        4) ê° ë¶„í¬ì˜ **ì§€ìˆ˜ ë¶€ë¶„**ë§Œ ì¶”ì¶œí•´ì„œ ê³±í•˜ë©´ $ \\exp \\left(-\\frac{\\left(X^{(t)}-\\sqrt{1-\\beta_t} X^{(t-1)}\\right)^2}{2 \\beta_t}\\right) \\times \\exp \\left(-\\frac{X_{t-1}^2}{2 \\sigma_{t-1}^2}\\right) = \\exp \\left(-\\frac{\\left(X_t-\\sqrt{\\alpha_t} X_{t-1}\\right)^2}{2 \\beta_t}-\\frac{X_{t-1}^2}{2 \\sigma_{t-1}^2}\\right) $ ì™€ ê°™ì´ ì •ë¦¬ ê°€ëŠ¥.\n",
    "\n",
    "        5) `4)`ì‹ì˜ ì§€ìˆ˜ ë‚´ë¶€ë¥¼ ì „ê°œ ì‹œ $ -\\frac{\\left(X_t^2-2 X_t \\sqrt{\\alpha_t} X_{t-1}+\\alpha_t X_{t-1}^2\\right)}{2 \\beta_t}-\\frac{X_{t-1}^2}{2 \\sigma_{t-1}^2} $ ê°€ ë˜ê³ , $ x^{(t-1)} $ í•­ì„ ë¬¶ìœ¼ë©´ $ -\\frac{X_t^2}{2 \\beta_t}+X_t X_{t-1} \\frac{\\sqrt{\\alpha_t}}{\\beta_t}-X_{t-1}^2\\left(\\frac{\\alpha_t}{2 \\beta_t}+\\frac{1}{2 \\sigma_{t-1}^2}\\right) $ ì´ ëœë‹¤. \n",
    "        \n",
    "        6) ì´ë¥¼ ì™„ì „ì œê³±ì‹($(x^{(t-1)}-\\mu)^2$ í˜•íƒœ)ìœ¼ë¡œ ë§Œë“¤ê²Œ ë˜ë©´ $ -\\frac{1}{2}\\left(\\frac{\\alpha_t}{\\beta_t}+\\frac{1}{\\sigma_{t-1}^2}\\right)\\left(X_{t-1}-\\frac{\\frac{X_t \\sqrt{\\alpha_t}}{\\beta_t}}{\\frac{\\alpha_t}{\\beta_t}+\\frac{1}{\\sigma_{t-1}^2}}\\right)^2+\\text { (ìƒìˆ˜í•­) } $ ê°€ ë˜ê¸° ë•Œë¬¸ì— ì–»ì–´ë‚´ê³ ì í–ˆë˜ $\\mu = 1 / (\\frac{1}{\\sigma_{t-1}^2} + \\frac{\\alpha_t}{\\beta_t}) \\big( \\frac{X_t \\sqrt{1- \\beta_t }}{\\beta_t}\\big) $ ì™€ $ \\sigma^2 = 1 / (\\frac{1}{\\sigma_{t-1}^2} + \\frac{\\alpha_t}{\\beta_t}) $ ë¥¼ ì•Œì•„ë‚¼ ìˆ˜ ìˆìŒ\n",
    "\n",
    "    - êµ¬í˜„: ì‹œì (`t-1`) ì—ì„œì˜ ë°ì´í„°(`x`)ëŒ€í•´ Reverse diff. processì˜ í‰ê· (`Î¼_t-1`)ê³¼ í‘œì¤€í¸ì°¨(`Ïƒ_t-1`)ë¥¼ ê³„ì‚°\n",
    "\n",
    "        1) `mlp_conv_dense` ëª¨ë¸ìœ¼ë¡œë¶€í„° timstepì— ë”°ë¥¸ Reverseê³¼ì •ì—ì„œì˜  `Z_t`ë¥¼ ì–»ì–´ëƒ„. ì´ëŠ” `Î¼_t`ì™€ `Î²_t`ì˜ ê³„ìˆ˜ë¡œ ì´ë¤„ì ¸ìˆìŒ\n",
    "        \n",
    "        2) `get_beta_forward`ë¥¼ ì´ìš©í•´ ì‹œì  tì˜ forward diff Noise ê°•ë„(`Î²_t`)ë¥¼ ì–»ì–´ëƒ„(t ì‹œì ì˜ betaê°’ ì™¸ì—ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ì´ë¤„ì§„ ì „ì²´ timestep ê¸¸ì´ë§Œí¼ì˜ 1ì°¨ì› í…ì„œ)\n",
    "        \n",
    "        3) mlpì—ì„œ ì¶”ì¶œí•œ `beta_coeff`ë¥¼  ì „ì²´ timestep ê¸¸ì´ì— ë§ê²Œ ìŠ¤ì¼€ì¼ë§(ì •ê·œí™”)ëœ `beta_coeff_scaled`ì— log(`beta_forward`)ë¥¼ ë”í•œ ê°’ì„ sigmoid í•¨ìˆ˜ì— í†µê³¼ì‹œì¼œ Reverse Diff. ì—ì„œ ì‚¬ìš©í•  `Î²_t`ê°’ì„ ì–»ì–´ëƒ„\n",
    "\n",
    "        4) bayes theoremì— ë”°ë¼ ì–»ì–´ì§„ porterior í™•ë¥ ë¶„í¬ì˜ `Î¼_t-1`ë¥¼ ê³„ì‚°. ì—¬ê¸°ì„œ `X_noisy * sqrt(1 - beta_forward)` ë¶€ë¶„ì€ ë…¸ì´ì¦ˆê°€ í¬í•¨ëœ ì´ë¯¸ì§€ `X_t-1`ë¥¼, `mu_coeff * sqrt(beta_forward)`ëŠ” MLPì—ì„œ ê³„ì‚°í•œ `Î¼_t-1` ê³„ìˆ˜(ëª¨ë¸ì´ í•™ìŠµí•œ ê²°ê³¼)ë¥¼ ë°˜ì˜.\n",
    "\n",
    "        5) ìƒê¸° ìœ ë„ëœ ë‘ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ ê²°í•©ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì‚°( $Ïƒ^2_{t-1}$ )ì„ ê³„ì‚°. `beta_t-1`ê°’ì´ ì»¤ì§€ë©´ `Ïƒ_t-1`ë„ ì»¤ì§€ë©´ì„œ ìƒ˜í”Œë§ì˜ ëœë¤ì„±ì´ ì¦ê°€í•˜ê²Œ ë¨.\n",
    "\n",
    "        - ì •ë¦¬: ë² ì´ì¦ˆ ì •ë¦¬(ìƒê¸° ìœ ë„) ê¸°ë°˜ìœ¼ë¡œ Reverse Diff ê³¼ì •ì˜ t-1 ì‹œì ì˜ í‰ê·  ë° ë¶„ì‚°ì„ ë„ì¶œí–ˆê³ , ì´ë¥¼ í†µí•´ diffusion modelì´ ì—­ë°©í–¥ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³µì›í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„\n",
    "\n",
    "- `generate_forward_diffusion_sample`: Noising ê³¼ì •ì„ êµ¬í˜„í•˜ê³ , ì´ ë•Œì˜ `Î¼_t`ì™€  `Ïƒ_t`ë¥¼ ê³„ì‚°\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Diffusion ëª¨ë¸ì€ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì ì§„ì ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì¸ í™•ì‚°(ë˜ëŠ” ë””í“¨ì „) ê³¼ì •ì„ í•™ìŠµ.\n",
    "    íŠ¹íˆ ì—­í™•ì‚°(Reverse Diffusion) ê³¼ì •ì—ì„œ ìƒˆë¡œìš´ ìƒ˜í”Œì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë¨\n",
    "\n",
    "    Args:\n",
    "        n_layers_conv (int): ì»¨ë³¼ë£¨ì…˜ ê³„ì¸µì˜ ìˆ˜ (í•˜ìœ„ MLPì™€ í•¨ê»˜ ì‚¬ìš©ë˜ëŠ” ë ˆì´ì–´)\n",
    "        n_layers_dense_lower (int): í•˜ë¶€ MLPì—ì„œ ì‚¬ìš©í•  ì¸µì˜ ìˆ˜\n",
    "        n_layers_dense_upper (int): ìƒë¶€ MLPì—ì„œ ì‚¬ìš©í•  ì¸µì˜ ìˆ˜\n",
    "        n_hidden_conv (int): ê° ì»¨ë³¼ë£¨ì…˜ ê³„ì¸µì˜ ì€ë‹‰ ìœ ë‹› ìˆ˜\n",
    "        n_hidden_dense_lower (int): í•˜ë¶€ MLPì˜ ì€ë‹‰ ìœ ë‹› ìˆ˜\n",
    "        n_hidden_dense_lower_output (int): í•˜ë¶€ MLP ì¶œë ¥ í¬ê¸° (ê³µê°„ì ìœ¼ë¡œ ì „ì—­ ì •ë³´ ì¶”ì¶œ)\n",
    "        n_hidden_dense_upper (int): ìƒë¶€ MLPì˜ ì€ë‹‰ ìœ ë‹› ìˆ˜\n",
    "        spatial_width (int): ì…ë ¥ ì´ë¯¸ì§€ì˜ ê³µê°„ì  í¬ê¸°\n",
    "        n_colors (int): ì…ë ¥ ì´ë¯¸ì§€ì˜ ì±„ë„ ìˆ˜\n",
    "        n_scales (int): ê° ì»¨ë³¼ë£¨ì…˜ì—ì„œ ì‚¬ìš©í•  ìŠ¤ì¼€ì¼ ìˆ˜\n",
    "        n_temporal_basis (int): ì¶œë ¥ ì‹œê°„ì  ê³„ìˆ˜ì˜ ìˆ˜ (ì˜ˆ: muì™€ sigmaë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 spatial_width,  # ì´ë¯¸ì§€ì˜ ê³µê°„ í¬ê¸° (ì˜ˆ: 28x28)\n",
    "                 n_colors,  # ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ ì±„ë„ ìˆ˜ (ì˜ˆ: 3 for RGB)\n",
    "                 trajectory_length=1000,  # í™•ì‚°(ë””í“¨ì „) ê²½ë¡œì˜ ê¸¸ì´\n",
    "                 n_temporal_basis=10,  # ì‹œê°„ì  ê¸°ì € í•¨ìˆ˜ì˜ ê°œìˆ˜\n",
    "                 n_hidden_dense_lower=500,  # í•˜ìœ„ MLPì˜ ì€ë‹‰ì¸µ ìœ ë‹› ìˆ˜\n",
    "                 n_hidden_dense_lower_output=2,  # í•˜ìœ„ MLPì˜ ì¶œë ¥ í¬ê¸°\n",
    "                 n_hidden_dense_upper=20,  # ìƒìœ„ MLPì˜ ì€ë‹‰ì¸µ ìœ ë‹› ìˆ˜\n",
    "                 n_hidden_conv=20,  # ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì˜ ì€ë‹‰ ìœ ë‹› ìˆ˜\n",
    "                 n_layers_conv=4,  # ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬ì˜ ë ˆì´ì–´ ìˆ˜\n",
    "                 n_layers_dense_lower=4,  # í•˜ìœ„ MLPì˜ ë ˆì´ì–´ ìˆ˜\n",
    "                 n_layers_dense_upper=2,  # ìƒìœ„ MLPì˜ ë ˆì´ì–´ ìˆ˜\n",
    "                 n_t_per_minibatch=1,  # í•œ ë¯¸ë‹ˆë°°ì¹˜ì—ì„œì˜ íƒ€ì„ìŠ¤í… ìˆ˜\n",
    "                 n_scales=1,  # ìŠ¤ì¼€ì¼ ìˆ˜ (ìŠ¤ì¼€ì¼ ê³µê°„ì„ ë‹¤ë£¨ëŠ” ë°©ì‹)\n",
    "                 step1_beta=0.001,  # ì²« ë²ˆì§¸ ë² íƒ€ ê°’\n",
    "                 uniform_noise=0):  # ê· ì¼ ë¶„í¬ ì¡ìŒ\n",
    "        super(DiffusionModel, self).__init__()\n",
    "\n",
    "        # ì£¼ìš” íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”\n",
    "        self.spatial_width = spatial_width\n",
    "        self.n_colors = n_colors\n",
    "        self.trajectory_length = trajectory_length\n",
    "        self.n_temporal_basis = n_temporal_basis\n",
    "        self.n_t_per_minibatch = n_t_per_minibatch\n",
    "        self.uniform_noise = uniform_noise\n",
    "\n",
    "        # MLP ëª¨ë¸ ì •ì˜ (ì»¨ë³¼ë£¨ì…˜+MLP í˜¼í•© ì•„í‚¤í…ì²˜)\n",
    "        self.mlp = MLP_conv_dense(n_layers_conv, n_layers_dense_lower, n_layers_dense_upper,\n",
    "                                  n_hidden_conv, n_hidden_dense_lower, n_hidden_dense_lower_output,\n",
    "                                  n_hidden_dense_upper, spatial_width, n_colors, n_scales, n_temporal_basis)\n",
    "        \n",
    "        # ì‹œê°„ì  ê¸°ì € í•¨ìˆ˜ ìƒì„±\n",
    "        self.temporal_basis = self.generate_temporal_basis(trajectory_length, n_temporal_basis)\n",
    "        \n",
    "        # ë² íƒ€ ê°’ ê³„ì‚° (í™•ì‚° ê²½ë¡œì˜ noise ê°•ë„)\n",
    "        self.beta_arr = self.generate_beta_arr(step1_beta)\n",
    "\n",
    "    def generate_beta_arr(self, step1_beta):\n",
    "        \"\"\"\n",
    "        í™•ì‚° ê²½ë¡œì—ì„œ ë² íƒ€ ê°’ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.\n",
    "        ë² íƒ€ëŠ” ê° timestepë§ˆë‹¤ noiseì˜ ê°•ë„ë¥¼ ê²°ì •í•˜ë©°,\n",
    "        ì´ë¥¼ í†µí•´ ë°ì´í„°ê°€ ì ì§„ì ìœ¼ë¡œ í™•ì‚°ë˜ëŠ” ê³¼ì •ì„ ëª¨ë¸ë§í•¨.\n",
    "        \n",
    "        ìˆœì„œ:\n",
    "          1) ìµœì†Œ ë² íƒ€ê°’ ì´ˆê¸°í™” ë° timestepìˆ˜ì— ëŒ€í•´ min_beta_valë¡œ ì´ˆê¸°í™”\n",
    "          2) ì²« ë²ˆì§¸ íƒ€ì„ìŠ¤í…ì—ë§Œ step1_beta ì¶”ê°€(ìµœì†Œê°’ì¸ 1e-6ì´ ë„ˆë¬´ ì‘ìŒ)\n",
    "          3) ë² íƒ€ ë³€í™”ë¥¼ ê²°ì •í•˜ëŠ” ë³´ì •ê°’(beta_perturb) ê³„ì‚°\n",
    "          4) ë² íƒ€ì˜ ê¸°ì €ì„ ì„ ê³„ì‚°í•œ ë’¤ ë³´ì •ê°’ì„ ë”í•˜ê³  sigmoid í†µê³¼ \n",
    "          5) ìµœëŒ€/ìµœì†Œê°’ì„ ì§€í‚¤ë„ë¡ ë³´ì •í•˜ì—¬ 1ì°¨ì› í…ì„œë¡œ ë°˜í™˜\n",
    "          \n",
    "        Args:\n",
    "            step1 beta(t=1ì¼ë•Œ t=0ì¼ë•Œ ë³´ë‹¤ Noiseë¥¼ í™•ì‹¤íˆ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•¨)\n",
    "            \n",
    "        Returns:\n",
    "            ìµœì¢… ì¶œë ¥ í…ì„œ (batch, spatial_width, spatial_width, n_output)\n",
    "        \"\"\"\n",
    "        min_beta_val = 1e-6  # ìµœì†Œ ë² íƒ€ ê°’ ì´ˆê¸°í™”\n",
    "        # ëª¨ë“  timestep(trajectory_length)ì— ëŒ€í•´ min_beta_valë¡œ ì´ˆê¸°í™”\n",
    "        min_beta_values = np.ones((self.trajectory_length,)) * min_beta_val \n",
    "        min_beta_values[0] += step1_beta  # ì²« ë²ˆì§¸ íƒ€ì„ìŠ¤í…ì—ë§Œ step1_betaë¥¼ ì¶”ê°€\n",
    "                                          # t=0ë³´ë‹¤ í™•ì‹¤íˆ ë§ì€ ë…¸ì´ì¦ˆë¥¼ ì£¼ê¸° ìœ„í•´ ì ìš©\n",
    "        \n",
    "        # ë² íƒ€ ë³€í™”ë¥¼ ê²°ì •í•˜ëŠ” ë³´ì •ê°’(beta_perturb) ê³„ì‚°\n",
    "        # 1) ì‹œê°„ ê¸°ì € í•¨ìˆ˜(n_temporal_basis) ì— ë”°ë¥¸ ë² íƒ€ê°’ ë³€í™”(ê¸°ë³¸ê°’ = 0)\n",
    "        # TODO add beta_perturb_coefficients to the parameters to be learned\n",
    "        beta_perturb_coefficients_values = np.zeros((self.n_temporal_basis,))\n",
    "        \n",
    "        # 2) ì‹œê°„ ê¸°ì € í•¨ìˆ˜ì™€ ë² íƒ€ ë³€í™” ë³´ì •ê°’ì„ í•©ì‚°(dot product)\n",
    "        beta_perturb = torch.matmul(self.temporal_basis.T,\n",
    "                                    torch.tensor(beta_perturb_coefficients_values, dtype=torch.float32))\n",
    "        \n",
    "        # ë² íƒ€ì˜ ê¸°ì €ì„ ì„ ê³„ì‚°\n",
    "        # step ìˆ˜ì— ë”°ë¼ t=0 -> t=T ê¹Œì§€ ì„ í˜• ì¦ê°€í•˜ëŠ” ë°°ì—´ ìƒì„±\n",
    "        beta_baseline = 1. / np.linspace(self.trajectory_length, 2., self.trajectory_length)\n",
    "        beta_baseline_offset = torch.tensor(np.log(beta_baseline), dtype=torch.float32) # ë¡œê·¸ë¥¼ ì·¨í•´ ìŠ¤ì¼€ì¼ ì¡°ì •\n",
    "\n",
    "        # ìµœì¢… ë² íƒ€ ê°’ ê³„ì‚° (sigmoid í•¨ìˆ˜ë¥¼ í†µí•´ ìŠ¤ì¼€ì¼ë§)\n",
    "        # ë² íƒ€ ê¸°ì €ê°’ì— ë³´ì •ê°’ì„ ë”í•´ sigmoid í•¨ìˆ˜ì— í†µê³¼(0~1ì‚¬ì´ê°’ìœ¼ë¡œ ë³€í™˜)\n",
    "        beta_arr = torch.sigmoid(beta_perturb + beta_baseline_offset)\n",
    "        # sigmoid í†µê³¼ í›„ ìµœì†Œ ë² íƒ€ê°’ ë° ìµœëŒ€ê°’(1 ë¯¸ë§Œ)ì´ ë˜ë„ë¡ ë³´ì •\n",
    "        beta_arr = min_beta_val + beta_arr * (1 - min_beta_val - 1e-5) \n",
    "        return beta_arr.view(self.trajectory_length, 1) # tì— ë”°ë¥¸ ë² íƒ€ê°’ì„ 1ì°¨ì› í…ì„œë¡œ ë°˜í™˜\n",
    "\n",
    "    def get_t_weights(self, t):\n",
    "        \"\"\"\n",
    "        ì£¼ì–´ì§„ timestep(t)ì— ëŒ€í•´ ë‹¤ë¥¸ tì™€ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜.\n",
    "        ì…ë ¥ ì‹œì (t)ì„ ì œì™¸í•˜ê³ ëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ t ì‹œì ì˜ ì •ë³´ë§Œ ë½‘ì•„ë‚´ëŠ” ì—­í• ì„ ìˆ˜í–‰.\n",
    "        \n",
    "        ìˆœì„œ:\n",
    "          1) 0~(t-1)ê¹Œì§€ì˜ ë°°ì—´ ìƒì„±\n",
    "          2) ì…ë ¥ë°›ì€ tì™€ ë‹¤ë¥¸ stepë“¤ì˜ ì°¨ì´ë¥¼ ì ˆëŒ“ê°’ìœ¼ë¡œ ê³„ì‚° í›„ 1ì—ì„œ ëºŒ\n",
    "          3) ìµœëŒ€ê°’ì„ ì œì™¸í•˜ê³  ëª¨ë‘ 0ìœ¼ë¡œ ë³€ê²½(ì…ë ¥ë°›ì€ tì˜ Index = 1/ ë‚˜ë¨¸ì§€ëŠ” 0)\n",
    "          4) ì…ë ¥ëœ tê°’ì„ ì œì™¸í•œ ë‹¤ë¥¸ Indexì˜ ê°’ì´ 0ì¸ timestep ê¸¸ì´ë§Œí¼ì˜ ë°°ì—´ì„ ë°˜í™˜\n",
    "          \n",
    "        Args:\n",
    "            t(ì¶”ì¶œí•˜ê³ ì‹¶ì€ ì‹œì ì˜ timestep)\n",
    "        \n",
    "        Returns:\n",
    "            ì…ë ¥ë°›ì€ t ì‹œì ì„ ì œì™¸í•˜ê³  ëª¨ë‘ 0ì¸ timestep ìˆ˜ ë§Œí¼ì˜ 1ì°¨ì› í…ì„œ\n",
    "        \"\"\"\n",
    "        n_seg = self.trajectory_length # ì „ì²´ timestep ìˆ˜\n",
    "        # 0ë¶€í„° trajectory_length-1ê¹Œì§€ì˜ íƒ€ì„ìŠ¤í… ë°°ì—´ ì´ˆê¸°í™”\n",
    "        t_compare = torch.arange(n_seg, dtype=torch.float32).view(1, n_seg) \n",
    "        # ë§¤ê°œë³€ìˆ˜ tì™€ ëª¨ë“  timestepì˜ ì°¨ì´ë¥¼ ì ˆëŒ€ê°’ìœ¼ë¡œ ê³„ì‚°\n",
    "        diff = torch.abs(t.view(1, 1) - t_compare)\n",
    "        # ì…ë ¥ë°›ì€ ì‹œì (t)ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§€ë„ë¡ í•¨\n",
    "        # ì…ë ¥ ì‹œì ì„ ì œì™¸í•˜ê³ ëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ get_beta_forward í•¨ìˆ˜ ë“±ì—ì„œ get_t_weights(t)ë¥¼ ê³±í•˜ë©´ tì‹œì ì—ì„œì˜ ê°’ì„ ê°•ì¡°\n",
    "        t_weights = torch.max(torch.stack([(1 - diff).view(n_seg, 1), torch.zeros((n_seg, 1))]), dim=1)[0]\n",
    "        return t_weights.view(-1, 1) # 1ì°¨ì› ë°°ì—´ë¡œ ë°˜í™˜\n",
    "\n",
    "    def get_beta_forward(self, t):\n",
    "        \"\"\"\n",
    "        get_t_weightsë¥¼ ì´ìš©í•˜ì—¬ íŠ¹ì • timestep(t)ì— í•´ë‹¹í•˜ëŠ” ë² íƒ€ ê°’ì„ ê³„ì‚°í•˜ê³  ë°˜í™˜í•¨.\n",
    "        í•™ìŠµ ê³¼ì •ì—ì„œ ì‚¬ìš©í•¨.\n",
    "        \n",
    "        Args:\n",
    "          t(betaê°’ì„ ì–»ê³ ì í•˜ëŠ” ì‹œì (t))\n",
    "        \n",
    "        Returns:\n",
    "          t ì‹œì ì˜ Beta ì™¸ì—ëŠ” 0ìœ¼ë¡œ ì´ë¤„ì§„ timestep ìˆ˜ ë§Œí¼ì˜ 1ì°¨ì› í…ì„œ\n",
    "        \"\"\"\n",
    "        t_weights = self.get_t_weights(t) # t ì‹œì ì„ ì œì™¸í•˜ê³  ëª¨ë‘ 0ì¸ timestep ìˆ˜ ë§Œí¼ì˜ 1ì°¨ì› í…ì„œ ë°˜í™˜\n",
    "        return torch.matmul(t_weights.T, self.beta_arr) # t ì‹œì ì˜ Beta ì™¸ì—ëŠ” 0ìœ¼ë¡œ ì´ë¤„ì§„ timestep ìˆ˜ ë§Œí¼ì˜ 1ì°¨ì› í…ì„œ ë°˜í™˜\n",
    "\n",
    "    def get_mu_sigma(self, X_noisy, t):\n",
    "        \"\"\"\n",
    "        MLPë¥¼ í†µí•´ ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì´ë¯¸ì§€ X_noisyì™€ íƒ€ì„ìŠ¤í… tì— ëŒ€í•´ muì™€ sigmaë¥¼ ê³„ì‚°.\n",
    "        ê³„ì‚°ëœ muì™€ sigmaì— ë³´ì • ë¶„í¬ë¥¼ ë”í•´ forwardì— \"ê·¼ì‚¬\"ì‹œì¼œ \n",
    "        ì—­í™•ì‚° ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ê³„ì‚°.\n",
    "        \n",
    "        ìˆœì„œ:\n",
    "          1) forward processë¥¼ í†µí•´ ì–»ì–´ì§„ X_t ë¥¼ MLPì— í†µê³¼í•˜ì—¬ Reverse ê³¼ì •ì—ì„œì˜ mu_tì™€ beta_t ì˜ˆì¸¡ê°’ì„ ì–»ì–´ëƒ„ \n",
    "          2) forwardì—ì„œì˜ beta_tì— logë¥¼ ì·¨í•œ ê°’ + ì •ê·œí™”ëœ reverse Diffì—ì„œì˜ beta_t => sigmoidì— í†µê³¼ => í™•ë¥ ê°’(beta_reverse) ì–»ì–´ëƒ„\n",
    "          3) t ì—ì„œì˜ Noisingëœ ë°ì´í„° + MLPë¥¼ í†µí•´ ì–»ì–´ë‚¸ ë¶€ë¶„ì—ì„œ sigmaë¥¼ ê³„ì‚°\n",
    "          4) t ì—ì„œì˜ ë¶„ì‚°ì€ beta_tê°€ ì§€ë°°í•˜ë¯€ë¡œ, beta_tì— ë£¨íŠ¸ë¥¼ ì”Œì›Œ í‘œì¤€í¸ì°¨(sigma_t)ë¥¼ êµ¬í•¨\n",
    "        \n",
    "        Args:\n",
    "            Noiseê°€ ì¶”ê°€ëœ X_tì™€ ì‹œì (t)\n",
    "        Returns:\n",
    "            ë³´ì •ì— ì˜í•´ ì–»ì–´ì§„ muì™€ sigma\n",
    "        \"\"\"\n",
    "        Z = self.mlp(X_noisy)  # MLPë¥¼ í†µí•´ Reverse Diffì˜ mu, beta ê³„ì‚°\n",
    "        mu_coeff, beta_coeff = self.temporal_readout(Z, t)  # tì— í•´ë‹¹í•˜ëŠ” ì—ì„œ muì™€ betaë¥¼ ì œì™¸í•œ ì „ì²´ step ê¸¸ì´ì˜ 1ì°¨ì› í…ì„œ\n",
    "        beta_forward = self.get_beta_forward(t) # ì‹œì  tì—ì„œì˜ betaê°’ ì™¸ì—ëŠ” 0ìœ¼ë¡œ ì´ë¤„ì§„ timestep ìˆ˜ ë§Œí¼ì˜ 1ì°¨ì› í…ì„œ\n",
    "        # ì‹œê°„ì  ê¸°ì €ì—ì„œì˜ betaë¥¼ ì „ì²´ trajectory ê¸¸ì´ë¡œ ìŠ¤ì¼€ì¼ë§\n",
    "        beta_coeff_scaled = beta_coeff / torch.sqrt(torch.tensor(self.trajectory_length, dtype=torch.float32))\n",
    "        # sigmoid í•¨ìˆ˜ë¥¼ í†µí•´(0~1ì‚¬ì´ì˜ validí•œ ê°’) ì‹œì  tì—ì„œì˜ betaë¥¼ ì¡°ì •\n",
    "        # -> reverse diffusion ê³¼ì •ì—ì„œì˜ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ì„ ê²°ì •\n",
    "        beta_reverse = torch.sigmoid(beta_coeff_scaled + torch.log(beta_forward))\n",
    "        \n",
    "        # muì™€ sigma(ê·¼ì‚¬ì¹˜) ê³„ì‚°\n",
    "        mu = X_noisy * torch.sqrt(1 - beta_forward) + mu_coeff * torch.sqrt(beta_forward)\n",
    "        sigma = torch.sqrt(beta_reverse)\n",
    "        return mu, sigma\n",
    "\n",
    "    def generate_forward_diffusion_sample(self, X_noiseless):\n",
    "        \"\"\"\n",
    "        ì£¼ì–´ì§„ X_noiseless ì´ë¯¸ì§€ì— ëŒ€í•´ t ì‹œì ì— ëŒ€í•´ Noisingëœ forward diffusion ìƒ˜í”Œì„ ìƒì„±.\n",
    "        ë™ì‹œì— forward ê³¼ì •ì—ì„œì˜ mu_tì™€ sigma_të„ ê³„ì‚°.\n",
    "        \n",
    "        ìˆœì„œ:\n",
    "          1) ì…ë ¥ëœ ì´ë¯¸ì§€ë¥¼ (n_channels, w, h)í˜•íƒœë¡œ ë³€í™˜, \n",
    "          2) \n",
    "          \n",
    "        Args:\n",
    "          ì›ë³¸ ë°ì´í„°(X_0)  \n",
    "        Returns:\n",
    "          t ì‹œì ì—ì„œì˜ ë…¸ì´ì§• ëœ ì´ë¯¸ì§€\n",
    "          ì„ íƒí•œ timestep\n",
    "          Reverseì—ì„œ ì‚¬ìš©í•  í‰ê· (mu) ë° í‘œì¤€í¸ì°¨(sigma)\n",
    "        \"\"\"\n",
    "        X_noiseless = X_noiseless.view(-1, self.n_colors, self.spatial_width, self.spatial_width)\n",
    "        n_images = X_noiseless.size(0)\n",
    "        t = torch.floor(torch.rand(1) * (self.trajectory_length - 1) + 1)\n",
    "        t_weights = self.get_t_weights(t)\n",
    "\n",
    "        # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€\n",
    "        N = torch.normal(mean=torch.zeros((n_images, self.n_colors, self.spatial_width, self.spatial_width)),\n",
    "                         std=torch.ones((n_images, self.n_colors, self.spatial_width, self.spatial_width)))\n",
    "\n",
    "        beta_forward = self.get_beta_forward(t)\n",
    "        alpha_forward = 1. - beta_forward\n",
    "        alpha_arr = 1. - self.beta_arr\n",
    "        alpha_cum_forward_arr = torch.cumprod(alpha_arr, dim=0).view(self.trajectory_length, 1)\n",
    "        alpha_cum_forward = torch.matmul(t_weights.T, alpha_cum_forward_arr)\n",
    "        \n",
    "        beta_cumulative = 1. - alpha_cum_forward\n",
    "        beta_cumulative_prior_step = 1. - alpha_cum_forward / alpha_forward\n",
    "\n",
    "        X_uniformnoise = X_noiseless + (torch.rand_like(X_noiseless) - 0.5) * self.uniform_noise\n",
    "        X_noisy = X_uniformnoise * torch.sqrt(alpha_cum_forward) + N * torch.sqrt(1. - alpha_cum_forward)\n",
    "\n",
    "        # muì™€ sigma ê³„ì‚°\n",
    "        mu1_scl = torch.sqrt(alpha_cum_forward / alpha_forward)\n",
    "        mu2_scl = 1. / torch.sqrt(alpha_forward)\n",
    "        cov1 = 1. - alpha_cum_forward / alpha_forward\n",
    "        cov2 = beta_forward / alpha_forward\n",
    "        lam = 1. / cov1 + 1. / cov2\n",
    "        \n",
    "        mu = (X_uniformnoise * mu1_scl / cov1 + X_noisy * mu2_scl / cov2) / lam\n",
    "        sigma = torch.sqrt(1. / lam).view(1, 1, 1, 1)\n",
    "\n",
    "        return X_noisy, t, mu, sigma\n",
    "\n",
    "    def get_beta_full_trajectory(self):\n",
    "        \"\"\"\n",
    "        ì „ì²´ í™•ì‚° ê²½ë¡œì— ëŒ€í•œ ë² íƒ€ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        alpha_arr = 1. - self.beta_arr\n",
    "        beta_full_trajectory = 1. - torch.exp(torch.sum(torch.log(alpha_arr)))\n",
    "        return beta_full_trajectory\n",
    "\n",
    "    def get_negL_bound(self, mu, sigma, mu_posterior, sigma_posterior):\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ì˜ ì†ì‹¤ í•¨ìˆ˜ì¸ ìŒì˜ ë¡œê·¸ ìš°ë„(negative log likelihood)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "        ì´ëŠ” KL ë°œì‚°ê³¼ ì—”íŠ¸ë¡œí”¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë°”ìš´ë“œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        KL = torch.log(sigma) - torch.log(sigma_posterior) + (sigma_posterior ** 2 + (mu_posterior - mu) ** 2) / (2 * sigma ** 2) - 0.5\n",
    "        H_startpoint = (0.5 * (1 + np.log(2. * np.pi))).astype(torch.float32) + 0.5 * torch.log(self.beta_arr[0])\n",
    "        H_endpoint = (0.5 * (1 + np.log(2. * np.pi))).astype(torch.float32) + 0.5 * torch.log(self.get_beta_full_trajectory())\n",
    "        H_prior = (0.5 * (1 + np.log(2. * np.pi))).astype(torch.float32) + 0.5 * torch.log(torch.tensor(1.))\n",
    "        \n",
    "        negL_bound = KL * self.trajectory_length + H_startpoint - H_endpoint + H_prior\n",
    "        negL_gauss = (0.5 * (1 + np.log(2. * np.pi))).astype(torch.float32) + 0.5 * torch.log(torch.tensor(1.))\n",
    "        negL_diff = negL_bound - negL_gauss\n",
    "        L_diff_bits = negL_diff / torch.log(torch.tensor(2.))\n",
    "        L_diff_bits_avg = L_diff_bits.mean() * self.n_colors\n",
    "        return L_diff_bits_avg\n",
    "\n",
    "    def cost_single_t(self, X_noiseless):\n",
    "        \"\"\"\n",
    "        ì£¼ì–´ì§„ íƒ€ì„ìŠ¤í…ì— ëŒ€í•´ í•˜ë‚˜ì˜ ë¹„ìš©ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        X_noisy, t, mu_posterior, sigma_posterior = self.generate_forward_diffusion_sample(X_noiseless)\n",
    "        mu, sigma = self.get_mu_sigma(X_noisy, t)\n",
    "        negL_bound = self.get_negL_bound(mu, sigma, mu_posterior, sigma_posterior)\n",
    "        return negL_bound\n",
    "\n",
    "    def temporal_readout(self, Z, t):\n",
    "        \"\"\"\n",
    "        MLP ì¶œë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œê°„ì  ì •ë³´ë¥¼ ì½ì–´ë“¤ì´ëŠ” í•¨ìˆ˜.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        n_images = Z.size(0) # mlpì¶œë ¥ì˜ ì›ì†Œ ìˆ˜\n",
    "        t_weights = self.get_t_weights(t) # \n",
    "        Z = Z.view(n_images, self.spatial_width, self.spatial_width, self.n_colors, 2, self.n_temporal_basis)\n",
    "        coeff_weights = torch.matmul(self.temporal_basis, t_weights)\n",
    "        concat_coeffs = torch.matmul(Z, coeff_weights)\n",
    "        mu_coeff = concat_coeffs[:, :, :, :, 0].permute(0, 3, 1, 2)\n",
    "        beta_coeff = concat_coeffs[:, :, :, :, 1].permute(0, 3, 1, 2)\n",
    "        return mu_coeff, beta_coeff\n",
    "\n",
    "    def generate_temporal_basis(self, trajectory_length, n_basis):\n",
    "        \"\"\"\n",
    "        ì‹œê°„ì  ê¸°ì € í•¨ìˆ˜(temporal basis)ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.\n",
    "        \"\"\"\n",
    "        temporal_basis = np.zeros((trajectory_length, n_basis))\n",
    "        xx = np.linspace(-1, 1, trajectory_length)\n",
    "        x_centers = np.linspace(-1, 1, n_basis)\n",
    "        width = (x_centers[1] - x_centers[0]) / 2.\n",
    "        for ii in range(n_basis):\n",
    "            temporal_basis[:, ii] = np.exp(-(xx - x_centers[ii]) ** 2 / (2 * width ** 2))\n",
    "        temporal_basis /= np.sum(temporal_basis, axis=1).reshape((-1, 1))\n",
    "        temporal_basis = temporal_basis.T\n",
    "        return torch.tensor(temporal_basis, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, X_noiseless):\n",
    "        \"\"\"\n",
    "        ì£¼ì–´ì§„ X_noiseless ì´ë¯¸ì§€ì— ëŒ€í•´ ì „ì²´ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ ì†ì‹¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        cost = 0.\n",
    "        for ii in range(self.n_t_per_minibatch):\n",
    "            cost += self.cost_single_t(X_noiseless)\n",
    "        return cost / self.n_t_per_minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¹ Argument Parser\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch-size', default=512, type=int, help='Batch size')\n",
    "    parser.add_argument('--lr', default=1e-3, type=float, help='Initial learning rate')\n",
    "    parser.add_argument('--resume_file', default=None, type=str, help='Saved model to continue training')\n",
    "    parser.add_argument('--suffix', default='', type=str, help='Optional suffix for model')\n",
    "    parser.add_argument('--output-dir', type=str, default='./', help='Output directory')\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='Number of training epochs')\n",
    "    parser.add_argument('--dropout_rate', type=float, default=0., help='Dropout rate')\n",
    "    parser.add_argument('--dataset', type=str, default='MNIST', help='Dataset to use')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "# ğŸ”¹ ë°ì´í„°ì…‹ ë¡œë”© í•¨ìˆ˜\n",
    "def get_dataloader(dataset_name, batch_size):\n",
    "    if dataset_name == 'MNIST':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))  # MNISTëŠ” í‘ë°±ì´ë¯€ë¡œ ì±„ë„ 1ê°œ\n",
    "        ])\n",
    "        train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "        test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "        n_colors, spatial_width = 1, 28\n",
    "    elif dataset_name == 'CIFAR10':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR10ì€ RGB\n",
    "        ])\n",
    "        train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "        test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "        n_colors, spatial_width = 3, 32\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset: {}\".format(dataset_name))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, n_colors, spatial_width\n",
    "\n",
    "# ğŸ”¹ í•™ìŠµ í•¨ìˆ˜\n",
    "def train_model(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ë°ì´í„° ë¡œë”©\n",
    "    train_loader, test_loader, n_colors, spatial_width = get_dataloader(args.dataset, args.batch_size)\n",
    "\n",
    "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    dpm = DiffusionModel(spatial_width, n_colors, uniform_noise=1./255.).to(device)\n",
    "    \n",
    "    # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜\n",
    "    optimizer = optim.RMSprop(dpm.parameters(), lr=args.lr)\n",
    "    criterion = nn.MSELoss()  # ë…¼ë¬¸ì— ë”°ë¼ ì†ì‹¤í•¨ìˆ˜ë¥¼ ë§ì¶°ì•¼ í•¨\n",
    "\n",
    "    # ëª¨ë¸ ì´ì–´ì„œ í•™ìŠµí•˜ê¸°\n",
    "    if args.resume_file:\n",
    "        print(f\"Loading checkpoint: {args.resume_file}\")\n",
    "        dpm.load_state_dict(torch.load(args.resume_file, map_location=device))\n",
    "\n",
    "    # ğŸ”¹ í•™ìŠµ ë£¨í”„\n",
    "    num_epochs = args.epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        dpm.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs, _ = batch  # MNIST/CIFAR10 ë°ì´í„°ëŠ” (image, label) í˜•ì‹\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = dpm.cost(inputs)  # ëª¨ë¸ì˜ cost() í•¨ìˆ˜ ì‚¬ìš©\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.6f}\")\n",
    "\n",
    "        # ëª¨ë¸ ì €ì¥\n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            save_path = os.path.join(args.output_dir, f'model_epoch{epoch+1}.pth')\n",
    "            torch.save(dpm.state_dict(), save_path)\n",
    "            print(f\"Model saved: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "junnyomii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
